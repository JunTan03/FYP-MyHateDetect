{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de1231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, precision_recall_curve, average_precision_score, roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, BertConfig, Trainer, TrainingArguments, EarlyStoppingCallback)\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import torch\n",
    "import nltk\n",
    "import malaya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f33f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "malay_stopwords = set(malaya.text.function.get_stopwords())\n",
    "custom_stopwords = {\"saya\", \"awak\", \"kau\", \"user\", \"en\", \"ms\", \"url\", \"je\", \"lah\", \"la\", \"number\", \"hahaha\", \"haha\", \"eh\"}\n",
    "all_stopwords = STOPWORDS.union(english_stopwords).union(malay_stopwords).union(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"./datasets/stage1/v2\")\n",
    "df = pd.read_csv(base_dir / \"stage1_combined_en_ms.csv\")\n",
    "df['text'] = df.apply(lambda row: f\"[MS] {row['text']}\" if row['lang'] == 'ms' else f\"[EN] {row['text']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = pd.Series(np.load(\"s1_db_visuals/train_texts.npy\", allow_pickle=True))\n",
    "train_labels = pd.Series(np.load(\"s1_db_visuals/train_labels.npy\", allow_pickle=True))\n",
    "val_texts   = pd.Series(np.load(\"s1_db_visuals/val_texts.npy\", allow_pickle=True))\n",
    "val_labels  = pd.Series(np.load(\"s1_db_visuals/val_labels.npy\", allow_pickle=True))\n",
    "\n",
    "test_texts  = pd.Series(np.load(\"s1_db_visuals/test_texts.npy\", allow_pickle=True))\n",
    "test_labels = np.load(\"s1_db_visuals/test_labels.npy\")\n",
    "test_langs  = pd.Series(np.load(\"s1_db_visuals/test_langs.npy\", allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f1042",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(list(map(str, examples[\"text\"])), padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "def prepare_dataset(texts, labels):\n",
    "    dataset = Dataset.from_pandas(pd.DataFrame({\"text\": texts, \"labels\": labels}))\n",
    "    dataset = dataset.map(tokenize_function, batched=True)\n",
    "    dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return dataset\n",
    "\n",
    "train_dataset = prepare_dataset(train_texts, train_labels)\n",
    "val_dataset = prepare_dataset(val_texts, val_labels)\n",
    "test_dataset = prepare_dataset(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", config=config)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33eb08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, preds, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./training_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=6,\n",
    "    warmup_ratio=0.1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee8c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fcc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Inference with Timing ===\n",
    "start_time = time.time()\n",
    "preds = trainer.predict(test_dataset)\n",
    "inference_time = (time.time() - start_time) / len(test_dataset)\n",
    "\n",
    "print(f\"\\n Inference Time per Sample: {inference_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d76353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Visualisation Output Placeholder ===\n",
    "output_dir = Path(\"./s1_mb_visuals\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and binarize\n",
    "y_true = preds.label_ids\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true_bin = label_binarize(y_true, classes=[0, 1]).ravel()\n",
    "y_score = preds.predictions[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3150049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for evaluation\n",
    "df_eval = pd.DataFrame({\n",
    "    \"text\": test_texts.reset_index(drop=True),\n",
    "    \"true\": y_true,\n",
    "    \"pred\": y_pred,\n",
    "    \"lang\": test_langs.reset_index(drop=True)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Confusion Matrix (Overall) ===\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=[\"Non-Hate\", \"Hate\"], yticklabels=[\"Non-Hate\", \"Hate\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"confusion_matrix.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b1d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Language-Specific Confusion Matrices ===\n",
    "langs = df_eval[\"lang\"].unique()\n",
    "for lang in langs:\n",
    "    sub = df_eval[df_eval[\"lang\"] == lang]\n",
    "    cm_lang = confusion_matrix(sub[\"true\"], sub[\"pred\"])\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm_lang, annot=True, fmt='d', cmap=\"Oranges\", xticklabels=[\"Non-Hate\", \"Hate\"], yticklabels=[\"Non-Hate\", \"Hate\"])\n",
    "    plt.title(f\"Confusion Matrix - {lang.upper()}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / f\"confusion_matrix_{lang}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee11e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_true_bin, y_score)\n",
    "auc_score = roc_auc_score(y_true_bin, y_score)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"roc_curve.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66eb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Precision-Recall Curve ===\n",
    "precision, recall, _ = precision_recall_curve(y_true_bin, y_score)\n",
    "ap = average_precision_score(y_true_bin, y_score)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, label=f'AP={ap:.2f}')\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"precision_recall_curve.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f37880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Word Clouds for FP & FN ===\n",
    "fp_texts = df_eval[(df_eval[\"true\"] == 0) & (df_eval[\"pred\"] == 1)][\"text\"]\n",
    "fn_texts = df_eval[(df_eval[\"true\"] == 1) & (df_eval[\"pred\"] == 0)][\"text\"]\n",
    "\n",
    "wordcloud_fp = WordCloud(width=800, height=400, stopwords=all_stopwords).generate(\" \".join(fp_texts))\n",
    "wordcloud_fn = WordCloud(width=800, height=400, stopwords=all_stopwords).generate(\" \".join(fn_texts))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(wordcloud_fp, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"False Positives Word Cloud\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"wordcloud_fp.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(wordcloud_fn, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"False Negatives Word Cloud\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"wordcloud_fn.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e11e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Classification Report ===\n",
    "report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "pd.DataFrame(report_dict).to_csv(output_dir / \"classification_report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf065145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Language-Specific Classification Reports ===\n",
    "lang_reports = {}\n",
    "for lang in langs:\n",
    "    sub = df_eval[df_eval[\"lang\"] == lang]\n",
    "    report = classification_report(sub[\"true\"], sub[\"pred\"], output_dict=True)\n",
    "    pd.DataFrame(report).to_csv(output_dir / f\"classification_report_{lang}.csv\")\n",
    "    lang_reports[lang] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb73ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === F1 Score by Class Bar Chart ===\n",
    "f1_scores = {label: report_dict[str(label)]['f1-score'] for label in [0, 1]}\n",
    "plt.bar([\"Non-Hate\", \"Hate\"], f1_scores.values(), color=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.title(\"F1 Score by Class\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"f1_by_class.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fd2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Prediction Accuracy Breakdown Bar ===\n",
    "df_eval[\"correct\"] = df_eval[\"true\"] == df_eval[\"pred\"]\n",
    "outcome_counts = df_eval[\"correct\"].value_counts().rename({True: \"Correct\", False: \"Incorrect\"})\n",
    "outcome_counts.plot(kind=\"bar\", color=[\"green\", \"red\"])\n",
    "plt.title(\"Prediction Accuracy Breakdown\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.grid(axis=\"y\")\n",
    "plt.savefig(output_dir / \"prediction_accuracy_breakdown.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb56f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss Overfit/Underfit Plot ===\n",
    "log_history = trainer.state.log_history\n",
    "val_loss = [entry[\"eval_loss\"] for entry in log_history if \"eval_loss\" in entry]\n",
    "train_loss = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "min_len = min(len(train_loss), len(val_loss))\n",
    "train_loss = train_loss[:min_len]\n",
    "val_loss = val_loss[:min_len]\n",
    "epochs = list(range(1, min_len + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, marker='o', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, marker='o', label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"loss_overfit_underfit.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c359ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Inference Time ===\n",
    "with open(output_dir / \"inference_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Inference Time per Sample: {inference_time:.4f} seconds\")\n",
    "\n",
    "print(\"\\n All Stage 1 visualisation outputs saved to ./stage1_visuals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b61da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save the Best Model and Tokenizer ===\n",
    "model_path = Path(\"s1_mb_model\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save best model weights and tokenizer\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save predictions for visuals\n",
    "np.save(output_dir / \"y_pred_mb.npy\", y_pred)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
