{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9aa5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, roc_curve, auc,\n",
    "    multilabel_confusion_matrix, precision_recall_curve\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    BertConfig, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from datasets import Dataset\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb06887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "base_dir = Path(\"./notebooks/stage2/v2\")\n",
    "base_dir2 = Path(\"./datasets/stage2/v2\")\n",
    "combined_csv = base_dir2 / \"stage2_final_combined.csv\"\n",
    "\n",
    "# Splits and Model directories\n",
    "splits_dir = Path(\"./s2_split\")\n",
    "model_dir = Path(\"./s2_mb_model\")\n",
    "visual_dir = Path(\"./s2_mb_visual\")\n",
    "\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "visual_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LABELS (lowercase) ===\n",
    "label_cols = [\"Race\", \"Religion\", \"Gender\", \"Sexual_Orientation\"]\n",
    "ml_label_cols = [f\"label_{c}\" for c in label_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load saved splits (npy files)\n",
    "train_texts = np.load(splits_dir / \"train_texts.npy\", allow_pickle=True)\n",
    "train_labels = np.load(splits_dir / \"train_labels.npy\", allow_pickle=True)\n",
    "\n",
    "val_texts = np.load(splits_dir / \"val_texts.npy\", allow_pickle=True)\n",
    "val_labels = np.load(splits_dir / \"val_labels.npy\", allow_pickle=True)\n",
    "\n",
    "test_texts = np.load(splits_dir / \"test_texts.npy\", allow_pickle=True)\n",
    "test_labels = np.load(splits_dir / \"test_labels.npy\", allow_pickle=True)\n",
    "test_langs = np.load(splits_dir / \"test_langs.npy\", allow_pickle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798a82c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOKENIZER & DATASET FUNCTIONS ===\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "def tokenize(texts, labels):\n",
    "    enc = tokenizer(list(map(str, texts)), padding=\"max_length\", truncation=True, max_length=128)\n",
    "    return Dataset.from_dict({\n",
    "        \"input_ids\": enc[\"input_ids\"],\n",
    "        \"attention_mask\": enc[\"attention_mask\"],\n",
    "        \"labels\": [list(map(float, l)) for l in labels]\n",
    "    })\n",
    "\n",
    "train_dataset = tokenize(train_texts, train_labels)\n",
    "val_dataset   = tokenize(val_texts, val_labels)\n",
    "test_dataset  = tokenize(test_texts, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37810c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Compute pos_weight for multilabel BCE (same as Stage 2 MBERT code)\n",
    "total_samples = train_labels.shape[0] + val_labels.shape[0] + test_labels.shape[0]\n",
    "label_counts = train_labels.sum(axis=0) + val_labels.sum(axis=0) + test_labels.sum(axis=0)\n",
    "pos_weights = torch.tensor((total_samples - label_counts) / label_counts, dtype=torch.float)\n",
    "if torch.cuda.is_available():\n",
    "    pos_weights = pos_weights.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODEL DEFINITION ===\n",
    "config = BertConfig.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", num_labels=len(label_cols),\n",
    "    problem_type=\"multi_label_classification\", hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1\n",
    ")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\", config=config\n",
    ")\n",
    "if torch.cuda.is_available(): model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4fc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels_arr = eval_preds\n",
    "    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    preds_bin = (probs > 0.5).astype(int)\n",
    "    metrics = {f\"f1_{c}\": f1_score(labels_arr[:, i], preds_bin[:, i], zero_division=0)\n",
    "               for i, c in enumerate(label_cols)}\n",
    "    metrics[\"macro_f1\"] = f1_score(labels_arr, preds_bin, average=\"macro\", zero_division=0)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31afc5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAINING ARGUMENTS ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(model_dir / \"checkpoints\"), \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    logging_strategy=\"epoch\", \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6, \n",
    "    weight_decay=0.01, \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\", \n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(), \n",
    "    logging_dir=str(model_dir / \"logs\"),\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WeightedTrainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=train_dataset, eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca673a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN & MEASURE TIME ===\n",
    "start_time = time.time()\n",
    "trainer.train()\n",
    "train_time = time.time() - start_time\n",
    "print(f\"ðŸ•’ Stage 2 training time: {train_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92490d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREDICTION & INFERENCE TIME ===\n",
    "start_inf = time.time()\n",
    "preds_output = trainer.predict(test_dataset)\n",
    "inf_time = time.time() - start_inf\n",
    "inference_time_per_sample = inf_time / len(test_dataset)\n",
    "print(f\"ðŸ•’ Inference time per sample: {inference_time_per_sample:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract arrays\n",
    "logits = preds_output.predictions\n",
    "true_labels = preds_output.label_ids\n",
    "probs_array = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "\n",
    "optimal_thresholds = []\n",
    "for i, label in enumerate(label_cols):\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels[:, i], probs_array[:, i])\n",
    "    f1_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    best_thresh = thresholds[np.argmax(f1_scores)]\n",
    "    optimal_thresholds.append(best_thresh)\n",
    "    print(f\"âœ… {label}: Best threshold = {best_thresh:.2f}\")\n",
    "\n",
    "# Apply best thresholds to probs_array\n",
    "preds_bin = np.zeros_like(probs_array)\n",
    "for i, thresh in enumerate(optimal_thresholds):\n",
    "    preds_bin[:, i] = (probs_array[:, i] > thresh).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35752fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save arrays for reuse\n",
    "np.save(model_dir / \"y_true.npy\", true_labels)\n",
    "np.save(model_dir / \"y_pred.npy\", preds_bin)\n",
    "np.save(model_dir / \"probs.npy\", probs_array)\n",
    "np.save(model_dir / \"test_texts.npy\", test_texts)\n",
    "np.save(model_dir / \"test_langs.npy\", test_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956708c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREATE EVAL DATAFRAME ===\n",
    "df_eval = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"lang\": test_langs\n",
    "})\n",
    "for i, c in enumerate(label_cols):\n",
    "    df_eval[f\"true_{c}\"] = true_labels[:, i]\n",
    "    df_eval[f\"pred_{c}\"] = preds_bin[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bdbcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Classification Report\n",
    "report_dict = classification_report(\n",
    "    true_labels, preds_bin, target_names=label_cols, zero_division=0, output_dict=True\n",
    ")\n",
    "pd.DataFrame(report_dict).transpose().to_csv(visual_dir / \"classification_report_overall.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc41041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report by Language\n",
    "for lang in df_eval[\"lang\"].unique():\n",
    "    subset = df_eval[df_eval[\"lang\"] == lang]\n",
    "    y_true_lang = subset[[f\"true_{c}\" for c in label_cols]].values\n",
    "    y_pred_lang = subset[[f\"pred_{c}\" for c in label_cols]].values\n",
    "    rep_lang = classification_report(\n",
    "        y_true_lang, y_pred_lang, target_names=label_cols, zero_division=0, output_dict=True\n",
    "    )\n",
    "    pd.DataFrame(rep_lang).transpose().to_csv(visual_dir / f\"classification_report_{lang}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves (Overall)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, c in enumerate(label_cols):\n",
    "    fpr, tpr, _ = roc_curve(true_labels[:, i], probs_array[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"{c} (AUC = {auc(fpr, tpr):.2f})\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.title(\"ROC Curves (Overall)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"roc_curves_overall.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cd9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves (Overall)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, c in enumerate(label_cols):\n",
    "    p, r, _ = precision_recall_curve(true_labels[:, i], probs_array[:, i])\n",
    "    plt.plot(r, p, label=c)\n",
    "plt.title(\"Precision-Recall Curves (Overall)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"pr_curves_overall.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration Curves (Overall)\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i, c in enumerate(label_cols):\n",
    "    frac_pos, mean_pred = calibration_curve(\n",
    "        true_labels[:, i], probs_array[:, i], n_bins=10, strategy=\"quantile\"\n",
    "    )\n",
    "    plt.plot(mean_pred, frac_pos, marker=\"o\", label=c)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.title(\"Calibration Curves (Overall)\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"calibration_curves_overall.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29662582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel Confusion Matrices (Overall)\n",
    "mcm = multilabel_confusion_matrix(true_labels, preds_bin)\n",
    "fig, axes = plt.subplots(1, len(label_cols), figsize=(16, 4))\n",
    "for i, (ax, c) in enumerate(zip(axes, label_cols)):\n",
    "    sns.heatmap(mcm[i], annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(c)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"confusion_matrices_overall.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66cf91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Combine stopwords\n",
    "eng_stop = set(stopwords.words(\"english\"))\n",
    "custom_stop = {\n",
    "    \"saya\", \"awak\", \"kau\", \"kita\", \"kamu\", \"dia\", \"mereka\", \"kita\", \"kami\",\n",
    "    \"yang\", \"itu\", \"ini\", \"dan\", \"atau\", \"dengan\", \"dalam\", \"kepada\", \"untuk\",\n",
    "    \"akan\", \"telah\", \"boleh\", \"tidak\", \"hanya\", \"lagi\", \"kerana\", \"jika\",\n",
    "    \"oleh\", \"pada\", \"sebagai\", \"adalah\", \"apa\", \"semua\", \"daripada\", \"lebih\",\n",
    "    \"perlu\", \"juga\", \"sudah\", \"masih\", \"pun\", \"satu\", \"ini\", \"mana\", \"setiap\",\n",
    "    \"tiada\", \"seorang\", \"bagaimana\", \"kenapa\", \"jadi\", \"akan\", \"mungkin\", \"mereka\",\n",
    "    \"dalam\", \"dengan\", \"untuk\", \"mempunyai\", \"anda\", \"user\", \"number\", \"url\", \"menjadi\", \"dari\",\n",
    "    \"tetapi\", \"bahawa\", \"seperti\", \"di\", \"sangat\", \"ada\", \"apabila\", \"ia\"\n",
    "}\n",
    "all_stopwords = set(STOPWORDS).union(eng_stop).union(custom_stop)\n",
    "\n",
    "# 2) Load test_texts array\n",
    "test_texts = np.load(\"s2_split/test_texts.npy\", allow_pickle=True)\n",
    "\n",
    "# 3) Word Cloud (All Text)\n",
    "all_text = \" \".join(map(str, test_texts.tolist()))\n",
    "wc_all = WordCloud(\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    stopwords=all_stopwords,\n",
    "    background_color=\"white\"\n",
    ").generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc_all, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud (All Text)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"wordcloud_all_text.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0e6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Word Clouds by Label\n",
    "test_labels = np.load(\"s2_split/test_labels.npy\", allow_pickle=True)\n",
    "\n",
    "for i, c in enumerate(label_cols):\n",
    "    subset_indices = np.where(test_labels[:, i] == 1)[0]\n",
    "    subset_texts = [str(test_texts[idx]) for idx in subset_indices]\n",
    "    if subset_texts:\n",
    "        wc_label = WordCloud(\n",
    "            width=1200,\n",
    "            height=600,\n",
    "            stopwords=all_stopwords,\n",
    "            background_color=\"white\"\n",
    "        ).generate(\" \".join(subset_texts))\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wc_label, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Word Cloud â€“ {c}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(visual_dir / f\"wordcloud_{c}.png\")\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4442bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Macro-F1 by Language\n",
    "lang_scores = []\n",
    "for lang in df_eval[\"lang\"].unique():\n",
    "    sub = df_eval[df_eval[\"lang\"] == lang]\n",
    "    y_true_lang = sub[[f\"true_{c}\" for c in label_cols]].values\n",
    "    y_pred_lang = sub[[f\"pred_{c}\" for c in label_cols]].values\n",
    "    lang_scores.append((lang, f1_score(y_true_lang, y_pred_lang, average=\"macro\")))\n",
    "lang_df = pd.DataFrame(lang_scores, columns=[\"lang\", \"macro_f1\"])\n",
    "sns.barplot(data=lang_df, x=\"lang\", y=\"macro_f1\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Macro-F1 by Language\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"macro_f1_by_language.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Label Co-occurrence Heatmap\n",
    "co_matrix = test_labels.T.dot(test_labels)\n",
    "sns.heatmap(\n",
    "    co_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_cols,\n",
    "    yticklabels=label_cols\n",
    ")\n",
    "plt.title(\"Label Co-occurrence\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"label_cooccurrence.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Confidence KDE Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, c in enumerate(label_cols):\n",
    "    sns.kdeplot(probs_array[:, i], fill=True, label=c)\n",
    "plt.title(\"Confidence Distribution (Overall)\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(visual_dir / \"confidence_kde.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save inference time\n",
    "with open(visual_dir / \"inference_time.txt\", \"w\") as f:\n",
    "    f.write(f\"Inference time per sample: {inference_time_per_sample:.4f} seconds\\n\")\n",
    "\n",
    "print(\"Stage 2 visualisations saved under:\", visual_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85380049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Underfitting/Overfitting Loss Plot ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract training & validation losses from trainerâ€™s log_history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_loss = [entry[\"loss\"] for entry in log_history if \"loss\" in entry]\n",
    "val_loss   = [entry[\"eval_loss\"] for entry in log_history if \"eval_loss\" in entry]\n",
    "\n",
    "# Align lengths\n",
    "min_len = min(len(train_loss), len(val_loss))\n",
    "train_loss = train_loss[:min_len]\n",
    "val_loss   = val_loss[:min_len]\n",
    "epochs     = list(range(1, min_len + 1))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, marker=\"o\", label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss,   marker=\"o\", label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save to visual_dir\n",
    "plt.savefig(visual_dir / \"loss_overfit_underfit.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save the Best Model and Tokenizer ===\n",
    "model_path = Path(\"s2_mb_model\")\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save best model weights and tokenizer\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save thresholds for multi-label inference\n",
    "import json\n",
    "thresholds_dict = {label: float(thresh) for label, thresh in zip(label_cols, optimal_thresholds)}\n",
    "with open(model_path / \"thresholds.json\", \"w\") as f:\n",
    "    json.dump(thresholds_dict, f)\n",
    "\n",
    "# Save predictions for visuals\n",
    "np.save(visual_dir / \"y_pred_mb.npy\", preds_bin)\n",
    "\n",
    "print(f\"Model and tokenizer saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c8b1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
